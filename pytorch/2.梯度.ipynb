{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标量梯度自动计算\n",
    "通过`requires_grad=True`，torch将会记录该张量所有操作，可以在必要的时候通过调用`.backward()`得到梯度值。  \n",
    "然后在需要计算梯度时，调用`.backward()`，torch将会自动计算梯度，最终保存在`.grad`属性中。  \n",
    "如果不需要让torch在记录操作计算梯度，则可以调用`.detach()`方法停止torch的记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当对x标记`requires_grad=True`时，则所有对x的计算将会被追踪，上述计算中，最终$out=\\frac{3}{4}(x+2)^2$，当我们调用`out.backward()`时，pytorch就开始对每一步x的计算进行求导，比如$\\frac{dout}{dx}=\\frac{3}{2}(x+2)$，实际上**对于标量（out）来说**，当调用`out.backward()`，等同于`out.backward(torch.tensor(1.))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量梯度自动计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看一个复杂的求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 注意只有float类型的tensor才能自动求导\n",
    "x = torch.arange(0,3, requires_grad=True, dtype=torch.float)\n",
    "y = x**2 + x*2\n",
    "z = y.sum()\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 注意只有float类型的tensor才能自动求导\n",
    "x = torch.arange(0,3, requires_grad=True, dtype=torch.float)\n",
    "y = x**2 + x*2\n",
    "z = y.sum()\n",
    "y.backward(torch.tensor([1.,1.,1.]))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里注意到，我们通过`z.backward()`和`y.backward(torch.tensor([1.,1.,1.]))`得到的$\\frac{dz}{dx}$结果是一样的，那这里关键就是`backward`函数的参数了。  \n",
    "```python\n",
    "variable.backward(grad_variables=None, retain_graph=None,create_graph=None)\n",
    "```\n",
    "其中，`grad_variables`：其形状与`variable`一致，对于`z.backward()`，`grad_variables`相当于链式法则$\\frac{dz}{dx}=\\frac{dz}{dx} \\times grad\\_variables$，那从上面的式子中我们可以知道，$\\frac{dz}{dx}=\\frac{dy}{dx}$，因此我们传入[1.0,1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一节，我们得到的out是一个数，大多数情况下，我们会得到一个向量，那一个向量对一个向量求导的一个基本公式就是雅克比矩阵，对于$\\vec y=f(\\vec x)$来说，我们有$$\\frac{dy}{dx}=\\left[\n",
    " \\begin{matrix}\n",
    "   \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_1}{\\partial x_3} \\\\\n",
    "   \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_3} \\\\\n",
    "   \\frac{\\partial y_3}{\\partial x_1} & \\frac{\\partial y_3}{\\partial x_2} & \\frac{\\partial y_3}{\\partial x_3} \\\\\n",
    "  \\end{matrix}\n",
    "  \\right] \\tag{3}$$\n",
    "但是实际上，pytorch不会为你自动计算这个雅克比矩阵，我们看一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14., 98.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([[3., 4.]], dtype=torch.float, requires_grad=True)\n",
    "y = torch.zeros(1,2)\n",
    "y[0,0] = x[0,0] ** 2 + x[0, 1]\n",
    "y[0,1] = x[0,1] ** 3 + x[0, 0]\n",
    "out = 2 * y\n",
    "out.backward(torch.tensor([[1.,1.]]), retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码转为数学公式为：\n",
    "$$\n",
    "x = \\left[\n",
    " \\begin{matrix}\n",
    "  3 & 4 \\\\\n",
    "  \\end{matrix}\n",
    "  \\right]  \\\\\n",
    "  y = \\left[\n",
    " \\begin{matrix}\n",
    "  0 \\\\\n",
    "   0\n",
    "  \\end{matrix}\n",
    "  \\right] \\\\\n",
    "y =  \\left[ \n",
    "\\begin{matrix}\n",
    "  x[0,0]^2+x[0,1] \\\\\n",
    " x[0,1]^3 + x[0,0]\n",
    "  \\end{matrix}\n",
    "  \\right] \\\\\n",
    "  out = 2y\n",
    "$$\n",
    "\n",
    "上述代码第7行，如果我们直接调用`out.backward`，会直接报错。默认情况下，如果`out.backward()`参数为None，则只能是标量对矢量求导，在上式中$$\n",
    "x=\\left[\n",
    " \\begin{matrix}\n",
    "  x_1 & x_2 \\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "\n",
    "$$  \n",
    "out=\\left[\n",
    " \\begin{matrix}\n",
    "  2x_1^2 + 2x_2\\\\\n",
    "  2x_2^3 + 2x_1 \\\\\n",
    "  \\end{matrix}\n",
    "  \\right] \\\\\n",
    "\\frac{dout}{dx}=\\left[\n",
    " \\begin{matrix}\n",
    "  4x_1 & 2 \\\\\n",
    "   2 & 6x_2^2\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "我们将x代入上式可得\n",
    "$$  \n",
    "\\frac{dout}{dx}=\\left[\n",
    " \\begin{matrix}\n",
    "  12 & 2 \\\\\n",
    "  2 & 96\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "按照推导，我们应该得出$ \n",
    "\\frac{dout}{dx}=\\left[\n",
    " \\begin{matrix}\n",
    "  12 & 2 \\\\\n",
    "  2 & 96\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$这个结论，可是实际pytorch给的结果是$ \n",
    "\\frac{dout}{dx}=\\left[\n",
    " \\begin{matrix}\n",
    "  14 & 98 \\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$，在观察，我们给`out.backward`传的参数是`torch.tensor([[1.,1.]])`，**它和`out`维度一样**，且我们发现这两个$\\frac{dy}{dx} * torch.tensor([[1.,1.]])$刚好是pytorch给的结果。  \n",
    "所以如果需要完整的雅克比矩阵，那就需要我们手动传参了：\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.,  2.],\n",
       "        [ 2., 96.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "x.grad.zero_()\n",
    "out.backward(torch.tensor([[1.,0.]]), retain_graph=True)\n",
    "g1 = copy.deepcopy(x.grad)\n",
    "x.grad.zero_()\n",
    "out.backward(torch.tensor([[0.,1.]]), retain_graph=True)\n",
    "g2 = copy.deepcopy(x.grad)\n",
    "torch.cat((g1, g2), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到我们得到了完成的雅克比矩阵，这里有些细节要注意：\n",
    "1. retain_graph，pytorch是将所有操作构成图，每次计算图之前节点就会舍弃，所以这里要保留，否则下一次backward就会报错\n",
    "2. zero_() 是为了将之前图中节点信息清零，便于重新计算\n",
    "\n",
    "参考：[pytoch求导-博客园](https://www.cnblogs.com/JeasonIsCoding/p/10164948.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
